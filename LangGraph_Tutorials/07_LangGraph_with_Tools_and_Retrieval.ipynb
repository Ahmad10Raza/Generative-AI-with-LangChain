{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be262f57",
   "metadata": {},
   "source": [
    "# LangGraph with Tools and Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d9618",
   "metadata": {},
   "source": [
    "### ✅ Restored Data Context\n",
    "\n",
    "You're building a **LangGraph-based multi-agent web app using Streamlit** that accepts a software development task (e.g., “Scrape all links from a webpage”), then executes multiple AI agents (planner, researcher, coder, reviewer, tester) in a **graph-defined order** using LangChain and OpenAI models.\n",
    "\n",
    "You previously encountered and fixed:\n",
    "\n",
    "* KeyError due to output misreference\n",
    "* Issues with `.bind(system=...)` in `ChatOpenAI`\n",
    "* Required `docstring` in tools\n",
    "* Missing `state_schema` in `StateGraph`\n",
    "\n",
    "Now you're asking:\n",
    "\n",
    "\n",
    "\n",
    "### 📦 LangGraph + Tools + LangChain Toolkits — Explained\n",
    "\n",
    "LangGraph provides **structured control flow** for agents and chains in **graph-based logic**, while LangChain provides **modular components** (chains, tools, prompts, agents). Combining them lets you create advanced multi-agent systems.\n",
    "\n",
    "\n",
    "\n",
    "### 🛠️ What Are Tools in LangChain?\n",
    "\n",
    "**Tools** in LangChain are callable functions wrapped so that LLM agents can use them.\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "```\n",
    "\n",
    "These can be used by agents that support tool usage (e.g., `ChatOpenAI` with function calling).\n",
    "\n",
    "\n",
    "\n",
    "### 🔄 How LangGraph Works with Tools\n",
    "\n",
    "You can treat a `tool` like a **LangGraph node**, allowing agents to invoke tools during execution. Example use cases:\n",
    "\n",
    "* API calls\n",
    "* Calculations\n",
    "* File operations\n",
    "* Database lookups\n",
    "\n",
    "\n",
    "\n",
    "### 🔗 LangChain Toolkit Overview\n",
    "\n",
    "LangChain offers modular **building blocks**:\n",
    "\n",
    "| Toolkit        | Purpose                                               |\n",
    "| -------------- | ----------------------------------------------------- |\n",
    "| **Chains**     | Sequential tasks (LLM → prompt → output)              |\n",
    "| **Agents**     | Dynamic tools and reasoning via ReAct or OpenAI tools |\n",
    "| **Memory**     | Stateful interactions across steps                    |\n",
    "| **Tools**      | Plug-in functions like search, math, scrape           |\n",
    "| **ChatPrompt** | Template chat formatting                              |\n",
    "| **Retrievers** | For document-based QA                                 |\n",
    "\n",
    "\n",
    "\n",
    "### 🔁 LangGraph + LangChain Integration Flow\n",
    "\n",
    "**Typical pattern:**\n",
    "\n",
    "1. Use LangChain tools (`@tool`) for actions.\n",
    "2. Define agents in LangGraph as **nodes**.\n",
    "3. Attach tools to agents using LangChain's tool wrappers.\n",
    "4. Control execution using LangGraph’s graph structure.\n",
    "5. Use `invoke()` or `astream()` to run the graph.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Example Use Case\n",
    "\n",
    "Let’s say the user wants:\n",
    "\n",
    "> “Generate a script to scrape image URLs from a webpage.”\n",
    "\n",
    "**Agents & Tools Setup:**\n",
    "\n",
    "* **Planner**: breaks task into steps.\n",
    "* **Researcher**: finds best libraries.\n",
    "* **Coder**: writes code (optionally uses tools).\n",
    "* **Tool**: validates or executes code (e.g., \"run\\_code\").\n",
    "* **Reviewer**: refines code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e7dd1",
   "metadata": {},
   "source": [
    "### 📚 Adding **Retrieval-Augmented Generation (RAG)** to LangGraph\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** enhances LLM responses by combining:\n",
    "\n",
    "* **Retrieval**: fetch relevant documents from a knowledge base.\n",
    "* **Generation**: LLM generates an answer based on those docs.\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Why RAG with LangGraph?\n",
    "\n",
    "In a **LangGraph agent system**, RAG helps:\n",
    "\n",
    "* Agents (like Researcher, Coder) answer based on **custom docs**.\n",
    "* Provide **contextual grounding** to reduce hallucination.\n",
    "* Enable question-answering over **private or domain-specific data**.\n",
    "\n",
    "\n",
    "\n",
    "### 🧱 Core Components for RAG\n",
    "\n",
    "1. **Vector Store** (e.g., FAISS, Chroma, Weaviate): for storing document embeddings.\n",
    "2. **Embeddings Model**: converts text into vector form (e.g., `OpenAIEmbeddings`, `HuggingFaceEmbeddings`).\n",
    "3. **Retriever**: queries the vector DB to get relevant chunks.\n",
    "4. **Document Chain** or **RAG Chain**: formats input + retrieved data, feeds to LLM.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Integration Steps in LangGraph\n",
    "\n",
    "#### 1. **Load & Embed Documents**\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/dev_docs.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 2. **Create a Retrieval Chain (RAG Chain)**\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 3. **Use RAG Chain as a LangGraph Node**\n",
    "\n",
    "```python\n",
    "def research_node(state):\n",
    "    question = state[\"messages\"][-1].content\n",
    "    result = rag_chain.run(question)\n",
    "    state[\"messages\"].append(HumanMessage(content=result))\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "```\n",
    "\n",
    "Then connect it like:\n",
    "\n",
    "```python\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_edge(\"planner\", \"researcher\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Use Cases\n",
    "\n",
    "* 👨‍⚕️ Medical research agents reading journals\n",
    "* 📄 Legal assistant agents referencing case law\n",
    "* 🏢 Corporate AI agents trained on company SOPs or internal docs\n",
    "\n",
    "\n",
    "\n",
    "### 💡 Best Practices\n",
    "\n",
    "| Tip        | Description                                             |\n",
    "| ---------- | ------------------------------------------------------- |\n",
    "| Chunking   | Split docs into 500–1000 tokens for better relevance    |\n",
    "| Metadata   | Store source/file names for traceability                |\n",
    "| Filter     | Use filters in retriever if docs are large/multi-domain |\n",
    "| Updateable | Keep vectorstore updateable (e.g., `add_documents`)     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba058d",
   "metadata": {},
   "source": [
    "### 🔄 **Graph Nodes for Search, Vector Store Lookup, and Summarization in LangGraph**\n",
    "\n",
    "In LangGraph, each **node** represents a distinct **step** or **behavior** in the agent's reasoning pipeline. When integrating **search**, **vector store lookup**, and **summarization**, each of these can be designed as dedicated nodes.\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 What Are Nodes in LangGraph?\n",
    "\n",
    "- A **node** is a function or a chain that:\n",
    "  - Takes in a `state` (like message history or task info)\n",
    "  - Processes it\n",
    "  - Returns an updated state\n",
    "\n",
    "\n",
    "\n",
    "## 📌 1. **Search Node** (Web or API Search)\n",
    "\n",
    "### ✅ Purpose:\n",
    "To allow agents to fetch **real-time info** from the web or external APIs.\n",
    "\n",
    "### 🔧 Implementation:\n",
    "```python\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def web_search_tool(query: str) -> str:\n",
    "    # Custom search logic or API like SerpAPI, Tavily\n",
    "    return f\"Search results for: {query}\"  # Simulate results\n",
    "\n",
    "search_node = Tool.from_function(\n",
    "    func=web_search_tool,\n",
    "    name=\"WebSearch\",\n",
    "    description=\"Useful for retrieving real-time information from the internet.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 👇 In Graph:\n",
    "```python\n",
    "builder.add_node(\"search\", safe_node(search_node, \"Search\"))\n",
    "builder.add_edge(\"researcher\", \"search\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 📌 2. **Vector Store Lookup Node** (Retrieval / RAG)\n",
    "\n",
    "### ✅ Purpose:\n",
    "Fetch relevant documents from a **local or hosted vector database** using user queries.\n",
    "\n",
    "### 🔧 Implementation:\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "retriever = FAISS.load_local(\"vector_db\", OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "def vector_lookup(state):\n",
    "    query = state[\"messages\"][-1].content\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    results = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "    state[\"messages\"].append(HumanMessage(content=results))\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "```\n",
    "\n",
    "### 👇 In Graph:\n",
    "```python\n",
    "builder.add_node(\"vector_lookup\", vector_lookup)\n",
    "builder.add_edge(\"planner\", \"vector_lookup\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 📌 3. **Summarization Node**\n",
    "\n",
    "### ✅ Purpose:\n",
    "To generate a concise summary of long retrieved results or multi-turn conversation.\n",
    "\n",
    "### 🔧 Implementation:\n",
    "```python\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summary_chain = load_summarize_chain(ChatOpenAI(model=\"gpt-4\"), chain_type=\"stuff\")\n",
    "\n",
    "def summarization_node(state):\n",
    "    docs = state[\"messages\"][-1].content\n",
    "    summary = summary_chain.run([Document(page_content=docs)])\n",
    "    state[\"messages\"].append(HumanMessage(content=summary))\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "```\n",
    "\n",
    "### 👇 In Graph:\n",
    "```python\n",
    "builder.add_node(\"summarizer\", summarization_node)\n",
    "builder.add_edge(\"vector_lookup\", \"summarizer\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🔁 Example Pipeline:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "Start --> Planner\n",
    "Planner --> Search\n",
    "Search --> Vector_Lookup\n",
    "Vector_Lookup --> Summarizer\n",
    "Summarizer --> Coder\n",
    "Coder --> Reviewer --> Tester --> END\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 💡 Tips\n",
    "\n",
    "| Feature        | Suggestion                                      |\n",
    "|----------------|--------------------------------------------------|\n",
    "| Vector Store   | Use `Chroma`, `FAISS`, or `Weaviate` for local/dev |\n",
    "| Search Tool    | Use `Tavily`, `SerpAPI`, or `Bing Web Search`     |\n",
    "| Summarization  | Use `map_reduce` or `refine` for large docs       |\n",
    "| Streamlit UI   | Allow user to toggle: 🔍 Search / 📚 RAG / 📝 Summary |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
