{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Flan models are a family of large language models (LLMs) developed by Google AI, known for their **improved generalizability and performance on few-shot learning tasks**. They build upon the foundation of T5 models, but with additional fine-tuning and adaptations designed to enhance their versatility and capabilities.\n",
    "\n",
    "Here's a breakdown of key aspects of Google Flan models:\n",
    "\n",
    "**Core Features:**\n",
    "\n",
    "* **Instruction Fine-tuning:** Flan models are fine-tuned with diverse sets of instructions, making them more adept at understanding and responding to specific prompts and requirements.\n",
    "* **Improved Few-shot Learning:** They perform significantly better on tasks with minimal training data, enabling faster adaptation to new domains or contexts.\n",
    "* **Variety of Sizes:** Flan models come in various sizes, from smaller versions like FLAN-T5-XL to larger and more powerful models like Flan-UL2.\n",
    "\n",
    "**Benefits and Advantages:**\n",
    "\n",
    "* **More Versatile:** Flan models can handle a broader range of tasks compared to standard T5 models, making them more suitable for diverse applications.\n",
    "* **Efficient for New Tasks:** They require less training data for new tasks, saving time and resources compared to training a new model from scratch.\n",
    "* **Accessible for Many Users:** With various size options, Flan models cater to users with different computational resources and needs.\n",
    "\n",
    "**Applications and Use Cases:**\n",
    "\n",
    "* **Text Generation:** Flan models can generate creative text formats like poems, scripts, code, and musical pieces, with enhanced understanding of specific instructions.\n",
    "* **Question Answering:** They provide accurate and informative answers to questions, even on topics with limited training data.\n",
    "* **Conversational AI:** Flan models can power engaging chatbots and virtual assistants that adapt to user contexts and preferences.\n",
    "* **Content Curation and Personalization:** They can recommend content or tailor experiences based on user interests and previous interactions.\n",
    "\n",
    "**Resources and Further Exploration:**\n",
    "\n",
    "* **Flan-T5 Tutorial:** [https://www.datacamp.com/podcast/gpt-3-and-our-ai-powered-future](https://www.datacamp.com/podcast/gpt-3-and-our-ai-powered-future)\n",
    "* **Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning:** [https://analyticsindiamag.com/google-ai-introduces-flan-a-language-model-with-instruction-fine-tuning/](https://analyticsindiamag.com/google-ai-introduces-flan-a-language-model-with-instruction-fine-tuning/)\n",
    "* **Hugging Face Model Cards for Flan models:** [https://huggingface.co/google/flan-t5-large](https://huggingface.co/google/flan-t5-large)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan20B-UL2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan20B-UL2: Pushing the Bounds of Language Understanding\n",
    "\n",
    "Flan20B-UL2 is a cutting-edge member of the Google Flan family, boasting an impressive 20 billion parameters and an even more remarkable **UL2 architecture**. This advanced design makes it a true powerhouse in the realm of large language models (LLMs).\n",
    "\n",
    "**What sets Flan20B-UL2 apart?**\n",
    "\n",
    "* **UL2 Architecture:** This innovative architecture allows Flan20B-UL2 to process information more efficiently and effectively, leading to:\n",
    "    * **Enhanced reasoning and comprehension:** It can better understand complex relationships and concepts in language, leading to more insightful and nuanced responses.\n",
    "    * **Improved factual accuracy:** It can access and process vast amounts of information with greater precision, resulting in more reliable outputs.\n",
    "    * **Greater flexibility and adaptability:** It can adjust its reasoning and responses based on context and information, making it versatile across diverse tasks.\n",
    "\n",
    "* **20 Billion Parameters:** This sheer size grants Flan20B-UL2:\n",
    "    * **Advanced learning capabilities:** It can train on larger datasets and complex tasks, leading to superior performance on challenging problems.\n",
    "    * **Increased creativity and fluency:** It can generate more imaginative and natural language outputs, ideal for creative tasks like writing stories or poems.\n",
    "    * **Stronger few-shot learning:** It can learn quickly and effectively from limited data, making it adaptable to new domains and settings.\n",
    "\n",
    "**What are the potential applications of Flan20B-UL2?**\n",
    "\n",
    "* **Scientific research:** Its advanced reasoning and comprehension capabilities can be invaluable for analyzing complex scientific data, formulating hypotheses, and generating new research directions.\n",
    "* **Education and personalized learning:** Its ability to access and process vast amounts of information can be used to create personalized learning experiences that cater to individual student needs.\n",
    "* **Healthcare and medicine:** Its analytical power can be harnessed for complex medical diagnosis, drug discovery, and personalized treatment plans.\n",
    "* **Creative content creation:** Its language fluency and creativity can be used for generating engaging stories, poems, scripts, and other forms of art.\n",
    "\n",
    "**Of course, with such power comes responsibility. Important considerations are:**\n",
    "\n",
    "* **Bias and fairness:** It's crucial to ensure that Flan20B-UL2 is trained on diverse and unbiased data to avoid perpetuating harmful stereotypes or misinformation.\n",
    "* **Interpretability and explainability:** Understanding how this LLM arrives at its outputs is essential for building trust and ensuring responsible use.\n",
    "* **Accessibility and ethical considerations:** It's important to ensure that this technology is used for good and is accessible to a diverse range of individuals and communities.\n",
    "\n",
    "Flan20B-UL2 represents a significant leap forward in LLM technology, with the potential to revolutionize how we interact with and understand the world around us. However, it's crucial to approach this powerful tool with care and responsibility, ensuring it benefits all of humanity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackheart/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "flan_ul2 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-ul2\", \n",
    "    model_kwargs={\"temperature\":0.1,\n",
    "                  \"max_new_tokens\":256})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ConversationBufferMemory(max_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=flan_ul2, \n",
    "    verbose=True, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Root. What is your name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi, I am a bot. My name is Root.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there! I am Root. What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Root. What is your name?\n",
      "AI: Hi, I am a bot. My name is Root.\n",
      "Human: Hello, how are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am good. How can I help you?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Root. What is your name?\n",
      "AI: Hi, I am a bot. My name is Root.\n",
      "Human: Hello, how are you?\n",
      "AI: I am good. How can I help you?\n",
      "Human: My TV is broken. can you help fix it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, I can help you with that. What is the brand and model number of your TV?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My TV is broken. can you help fix it?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
